{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64da4166",
   "metadata": {},
   "source": [
    "# Environment check and imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ea5f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu118\n",
      "OpenCV version: 4.12.0\n",
      "CUDA available: True\n",
      "GPU device: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Core imports and environment check\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Running on CPU. For real-time performance, a GPU is recommended.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de075e",
   "metadata": {},
   "source": [
    "# UModel loading and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c222525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on: cuda\n",
      "Number of classes: 80\n"
     ]
    }
   ],
   "source": [
    "# Model configuration and loading\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_PATH = \"yolov8m.pt\"  # Use yolov8n.pt for faster but less accurate inference\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "model = YOLO(MODEL_PATH)\n",
    "model.to(DEVICE)\n",
    "\n",
    "print(f\"Model loaded on: {DEVICE}\")\n",
    "print(f\"Number of classes: {len(model.names)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a48b2ff",
   "metadata": {},
   "source": [
    "# Utility functions (centroid tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b74968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic centroid-based multi-object tracking\n",
    "track_history = defaultdict(list)\n",
    "object_positions = {}\n",
    "next_track_id = 0\n",
    "\n",
    "\n",
    "def compute_centroid(box):\n",
    "    \"\"\"\n",
    "    Compute the centroid of a bounding box.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    box : tuple or list (x1, y1, x2, y2)\n",
    "        Bounding box coordinates in pixel space.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Centroid as (cx, cy).\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = box\n",
    "    cx = (x1 + x2) / 2.0\n",
    "    cy = (y1 + y2) / 2.0\n",
    "    return np.array([cx, cy], dtype=np.float32)\n",
    "\n",
    "\n",
    "def associate_detections_to_tracks(detections, prev_positions, distance_threshold=50.0):\n",
    "    \"\"\"\n",
    "    Associate current detections with existing tracks using Euclidean distance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    detections : list\n",
    "        List of tuples (box, confidence, class_name).\n",
    "    prev_positions : dict\n",
    "        Mapping track_id -> last centroid position.\n",
    "    distance_threshold : float\n",
    "        Maximum allowed distance to keep a track assignment.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapping track_id -> (box, confidence, class_name, centroid).\n",
    "    \"\"\"\n",
    "    global next_track_id\n",
    "\n",
    "    assignments = {}\n",
    "    used_tracks = set()\n",
    "\n",
    "    for box, confidence, class_name in detections:\n",
    "        centroid = compute_centroid(box)\n",
    "        best_track_id = None\n",
    "        best_distance = distance_threshold\n",
    "\n",
    "        for track_id, prev_centroid in prev_positions.items():\n",
    "            if track_id in used_tracks:\n",
    "                continue\n",
    "\n",
    "            distance = np.linalg.norm(centroid - prev_centroid)\n",
    "            if distance < best_distance:\n",
    "                best_distance = distance\n",
    "                best_track_id = track_id\n",
    "\n",
    "        if best_track_id is not None:\n",
    "            assignments[best_track_id] = (box, confidence, class_name, centroid)\n",
    "            used_tracks.add(best_track_id)\n",
    "        else:\n",
    "            assignments[next_track_id] = (box, confidence, class_name, centroid)\n",
    "            next_track_id += 1\n",
    "\n",
    "    return assignments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61a7d31",
   "metadata": {},
   "source": [
    "# Webcam detection and tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f6da4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press ESC to exit the webcam window.\n"
     ]
    }
   ],
   "source": [
    "# Real-time object detection and tracking from webcam\n",
    "WEB_CAM_INDEX = 0  # Default webcam device\n",
    "\n",
    "cap = cv2.VideoCapture(WEB_CAM_INDEX)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Failed to open webcam. Check camera index or permissions.\")\n",
    "\n",
    "track_history.clear()\n",
    "object_positions.clear()\n",
    "\n",
    "print(\"Press ESC to exit the webcam window.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"No frame received from webcam. Exiting.\")\n",
    "        break\n",
    "\n",
    "    # YOLO inference\n",
    "    results = model(frame, device=DEVICE, verbose=False)\n",
    "\n",
    "    detections = []\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            confidence = float(box.conf[0].cpu().numpy())\n",
    "            class_id = int(box.cls[0].cpu().numpy())\n",
    "            class_name = model.names[class_id]\n",
    "\n",
    "            if confidence < CONFIDENCE_THRESHOLD:\n",
    "                continue\n",
    "\n",
    "            detections.append(\n",
    "                (\n",
    "                    [int(x1), int(y1), int(x2), int(y2)],\n",
    "                    confidence,\n",
    "                    class_name,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Tracking\n",
    "    assignments = associate_detections_to_tracks(detections, object_positions)\n",
    "    object_positions = {}\n",
    "\n",
    "    # Drawing\n",
    "    for track_id, (box, confidence, class_name, centroid) in assignments.items():\n",
    "        object_positions[track_id] = centroid\n",
    "        track_history[track_id].append(centroid)\n",
    "\n",
    "        x1, y1, x2, y2 = box\n",
    "        color = (0, 255, 0)\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "        label = f\"ID {track_id} | {class_name} {confidence:.2f}\"\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            label,\n",
    "            (x1, max(0, y1 - 10)),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            color,\n",
    "            2,\n",
    "            lineType=cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "        cv2.circle(frame, tuple(centroid.astype(int)), 4, (0, 0, 255), -1)\n",
    "\n",
    "        if len(track_history[track_id]) > 1:\n",
    "            points = np.array(track_history[track_id], dtype=np.int32)\n",
    "            cv2.polylines(frame, [points], isClosed=False, color=(0, 255, 255), thickness=2)\n",
    "\n",
    "    cv2.imshow(\"Webcam - Object Detection and Tracking\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27:  # ESC\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a9c6bc",
   "metadata": {},
   "source": [
    "# Video file detection and tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8304f3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video. Press ESC to stop early.\n",
      "End of video stream.\n",
      "Tracked video saved to: outputs/sample_01_tracked.mp4\n"
     ]
    }
   ],
   "source": [
    "# Object detection and tracking from a recorded video\n",
    "VIDEO_PATH = \"D:\\\\ai\\\\CodeAlpha\\\\Task_3_Object_Detection\\\\Test\\\\طلب_فيديو_بطيء_للكشف_عن_الوجوه.mp4\"  \n",
    "OUTPUT_PATH = \"outputs/sample_01_tracked.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Failed to open video file: {VIDEO_PATH}\")\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "track_history.clear()\n",
    "object_positions.clear()\n",
    "\n",
    "print(\"Processing video. Press ESC to stop early.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video stream.\")\n",
    "        break\n",
    "\n",
    "    results = model(frame, device=DEVICE, verbose=False)\n",
    "\n",
    "    detections = []\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            confidence = float(box.conf[0].cpu().numpy())\n",
    "            class_id = int(box.cls[0].cpu().numpy())\n",
    "            class_name = model.names[class_id]\n",
    "\n",
    "            if confidence < CONFIDENCE_THRESHOLD:\n",
    "                continue\n",
    "\n",
    "            detections.append(\n",
    "                (\n",
    "                    [int(x1), int(y1), int(x2), int(y2)],\n",
    "                    confidence,\n",
    "                    class_name,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    assignments = associate_detections_to_tracks(detections, object_positions)\n",
    "    object_positions = {}\n",
    "\n",
    "    for track_id, (box, confidence, class_name, centroid) in assignments.items():\n",
    "        object_positions[track_id] = centroid\n",
    "        track_history[track_id].append(centroid)\n",
    "\n",
    "        x1, y1, x2, y2 = box\n",
    "        color = (255, 0, 0)\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "        label = f\"ID {track_id} | {class_name} {confidence:.2f}\"\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            label,\n",
    "            (x1, max(0, y1 - 10)),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            color,\n",
    "            2,\n",
    "            lineType=cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "        cv2.circle(frame, tuple(centroid.astype(int)), 4, (0, 255, 255), -1)\n",
    "\n",
    "    writer.write(frame)\n",
    "    cv2.imshow(\"Video - Object Detection and Tracking\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27:  # ESC\n",
    "        print(\"Interrupted by user.\")\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "writer.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Tracked video saved to: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c5d75",
   "metadata": {},
   "source": [
    "# Real-Time Object Detection and Tracking with YOLO and OpenCV\n",
    "\n",
    "This notebook implements a real-time object detection and multi-object tracking system\n",
    "using a pretrained YOLO model and OpenCV. The application supports both live webcam\n",
    "input and recorded video files.\n",
    "\n",
    "The main objectives are:\n",
    "- Detect multiple objects in each video frame using a pretrained YOLO model.\n",
    "- Track detected objects across consecutive frames with a lightweight centroid-based tracker.\n",
    "- Visualize bounding boxes, labels, confidence scores, and track IDs on the video stream.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Project Overview\n",
    "\n",
    "The goal of this project is to build a practical vision pipeline that can be used\n",
    "for tasks such as basic surveillance, traffic analysis, or activity monitoring in a\n",
    "controlled environment. The focus is on clarity, real-time performance, and a clean\n",
    "implementation that can be extended later if needed.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Environment Setup\n",
    "\n",
    "This notebook assumes a Python environment with:\n",
    "- PyTorch (with CUDA support if a GPU is available)\n",
    "- OpenCV for video capture and visualization\n",
    "- Ultralytics YOLO for object detection\n",
    "\n",
    "A GPU is strongly recommended to maintain real-time performance, particularly when\n",
    "processing high-resolution video streams.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Model Loading and Configuration\n",
    "\n",
    "A pretrained YOLO model is used for object detection, loaded through the Ultralytics\n",
    "Python API. This interface simplifies running inference on images and video frames.\n",
    "\n",
    "Key configuration parameters:\n",
    "- `MODEL_PATH`: path to the pretrained YOLO weights (e.g., `yolov8n.pt` or `yolov8m.pt`).\n",
    "- `DEVICE`: computation device (`cuda` if available, otherwise `cpu`).\n",
    "- `CONFIDENCE_THRESHOLD`: minimum confidence required to accept a detection.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Tracking Logic\n",
    "\n",
    "To maintain object identities across frames, a simple centroid-based tracker is used:\n",
    "- The centroid of each detected bounding box is computed.\n",
    "- Current detections are matched to existing tracks based on Euclidean distance.\n",
    "- Each object is assigned a persistent track ID as long as it stays visible.\n",
    "\n",
    "This approach is lightweight, requires no additional training, and is suitable for\n",
    "scenes with moderate motion and limited occlusions.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Webcam Inference\n",
    "\n",
    "The notebook includes a section that:\n",
    "- Connects to the default webcam.\n",
    "- Runs YOLO inference on each incoming frame.\n",
    "- Applies the tracking logic.\n",
    "- Displays the result in an OpenCV window with bounding boxes, labels, and IDs.\n",
    "\n",
    "Press the `ESC` key in the OpenCV window to stop the webcam stream.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Video File Inference\n",
    "\n",
    "The same detection and tracking pipeline can be applied to recorded video files:\n",
    "- Frames are read sequentially from a video file.\n",
    "- Detections and tracking are computed for each frame.\n",
    "- The processed video is written to a new output file for later review.\n",
    "\n",
    "This mode is useful for offline analysis, demonstrations, or creating sample results.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Notes and Possible Extensions\n",
    "\n",
    "Notes:\n",
    "- Tune `CONFIDENCE_THRESHOLD` to balance between detection recall and precision.\n",
    "- Use lighter model variants or reduced frame resolution to increase throughput.\n",
    "\n",
    "Possible extensions:\n",
    "- Integrate a more advanced tracker such as SORT or Deep SORT.\n",
    "- Restrict processing to specific regions of interest (ROIs) in the frame.\n",
    "- Log and analyze trajectories, object counts, and dwell times for further analytics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43ec51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
